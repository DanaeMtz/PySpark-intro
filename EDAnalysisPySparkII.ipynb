{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in pySpark II "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up \n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = SparkContext(master = \"local\" \\\n",
    "                , appName = \"Exploratory Data Analysis\") \n",
    "\n",
    "# Create a spark session \n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files we will be working with\n",
    "path = \"/home/danae/Documents/pySparkTraining/files/\"\n",
    "\n",
    "df = spark.read.parquet(path + \"olimpic_medals.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "\n",
    "You have several options for visualizing and describing univariate data. Such as frequency Distribution Tables, bar Charts, histograms, frequency Polygons, pie Charts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age','height', 'weigth']\n",
    "df.select(num_cols).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find out that the default function in PySpark does not include the quartiles. The following function will help you to get the same results in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "def describe_pd(df_in, columns, deciles=False):\n",
    "    '''\n",
    "    Function to union the basic stats results and deciles\n",
    "    :param df_in: the input dataframe\n",
    "    :param columns: the cloumn name list of the numerical variable\n",
    "    :param deciles: the deciles output\n",
    "\n",
    "    :return : the numerical describe info. of the input dataframe\n",
    "\n",
    "    :author: Ming Chen and Wenqiang Feng\n",
    "    :email:  von198@gmail.com\n",
    "    '''\n",
    "\n",
    "    if deciles:\n",
    "        percentiles = np.array(range(0, 110, 10))\n",
    "    else:\n",
    "        percentiles = [25, 50, 75]\n",
    "\n",
    "    percs = np.transpose([np.percentile(df_in.select(x).collect(), percentiles) for x in columns])\n",
    "    percs = pd.DataFrame(percs, columns=columns)\n",
    "    percs['summary'] = [str(p) + '%' for p in percentiles]\n",
    "\n",
    "    spark_describe = df_in.describe().toPandas()\n",
    "    new_df = pd.concat([spark_describe, percs],ignore_index=True)\n",
    "    new_df = new_df.round(2)\n",
    "    return new_df[['summary'] + columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_pd(df, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, because of the confidential data issues, you can not deliver the real data and your clients may ask more statistical results, such as deciles. You can apply the follwing function to achieve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_pd(df, num_cols, deciles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, skewness, kurtosis\n",
    "df.select(skewness('age'),kurtosis('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import rank,sum,col\n",
    "from pyspark.sql import Window\n",
    "\n",
    "window = Window.rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "# withColumn('Percent %',F.format_string(\"%5.0f%%\\n\",col('Credit_num')*100/col('total'))).\\\n",
    "tab = df.select(['gender','weigth']).\\\n",
    "   groupBy('gender').\\\n",
    "   agg(F.count('weigth').alias('n'),\n",
    "       F.mean('weigth').alias('avg'),\n",
    "       F.min('weigth').alias('min'),\n",
    "       F.max('weigth').alias('max')).\\\n",
    "   withColumn('total', sum(col('n')).over(window)).\\\n",
    "   withColumn('Percent',col('n')*100/col('total')).\\\n",
    "   drop(col('total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis\n",
    "\n",
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "\n",
    "corr_data = df.select(num_cols)\n",
    "\n",
    "col_names = corr_data.columns\n",
    "features = corr_data.rdd.map(lambda row: row[0:])\n",
    "corr_mat = Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_mat)\n",
    "corr_df.index, corr_df.columns = col_names, col_names\n",
    "\n",
    "print(corr_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "#df = sns.load_dataset(\"iris\")\n",
    "df_plot = df['age','height', 'weigth', 'gender']\n",
    "df_plot.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_plot.toPandas(), hue = \"gender\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.crosstab('season', 'gender').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() # close the spark session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
